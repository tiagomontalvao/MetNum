{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinação de pontos em 1D/2D dadas distâncias entre pares\n",
    "\n",
    "Nos é apresentado o seguinte problema:\n",
    "\n",
    "*Dada uma matriz $d_{ij} \\in \\mathbb{R}^{n\\times n}$ indicando a distância entre cada par de pontos $i$ e $j$, o problema consiste em determinar pontos $p_1, p_2, \\dots, p_n \\in \\mathbb{R}^2$ tentando atender as restrições $\\|p_i-p_j\\| = d_{ij}, \\forall i, j$.*\n",
    "\n",
    "Resolver isto analiticamente consiste na resolução de um sistema não linear no qual aparecem termos de grau 2. Este sistema pode ser resolvido usando substituição ou algum método numérico. Vale adicionar uma sessão aqui no final do trabalho sobre esta abordagem, embora o problema a seguir não seja resolvível por este método.\n",
    "\n",
    "O problema completo então é o seguinte:\n",
    "\n",
    "*Dada uma matriz $d_{ij} \\in \\mathbb{R}^{n\\times n}$ indicando a distância entre cada par de pontos $i$ e $j$ em uma superfície **não necessariamente plana**, o problema consiste em determinar pontos $p_1, p_2, \\dots, p_n \\in \\mathbb{R}^2$ tais que os valores $\\|p_i-p_j\\|$ e $d_{ij}$ não sejam muito diferentes, para alguma noção de diferença.*\n",
    "\n",
    "Primeiro precisamos entender que temos $n$ pontos para escolher onde colocar no plano e que a escolha destes pontos apresenta certo grau de liberdade. De fato, dada uma configuração fixada para os pontos, qualquer translação ou aplicação linear que preserve produto interno (consequentemente preservando distâncias entre vetores, por exemplo matrizes ortogonais) resulta em outra configuração para os pontos com as mesmas distâncias entre os pontos. Mesmo não entendendo formalmente ainda o que estamos querendo descobrir, é evidente que existe certo grau de liberdade para a escolha dos pontos.\n",
    "\n",
    "Por isso, a partir de agora, consideraremos $p_1 = \\bar0$, ou seja, **$p_1$ está fixado na origem.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição da função de erro\n",
    "\n",
    "Precisamos agora entender melhor o que estamos querendo encontrar.\n",
    "\n",
    "Defina $p=(p_1, p_2, \\dots, p_n)$. Podemos começar pensando em definir o erro de uma configuração $p$ como o seguinte valor:\n",
    "\n",
    "$$\n",
    "\\displaystyle \\sum_i \\sum_{j>i} \\left|\\|p_i-p_j\\|-d_{ij}\\right|\n",
    "$$\n",
    "\n",
    "Esta quantidade é a soma para todo par de pontos da distância atribuída a eles pela escolha de suas coordenadas e o valor nos dado na entrada pela matriz $d_{ij}$.\n",
    "\n",
    "Esta quantidade é significativa, mas é péssima para realizar cálculos, pois apresenta normas de vetores sem estarem ao quadrado. Por isso, a função de erro adotada daqui para frente será a seguinte:\n",
    "\n",
    "$$\n",
    "e(p) = \\displaystyle \\sum_i \\sum_{j>i} \\left|\\|p_i-p_j\\|^2-d_{ij}^2\\right|^2\n",
    "$$\n",
    "\n",
    "Sendo assim, podemos escrever o seguinte problema de otimização, sabendo que $p_1 = \\bar0$:\n",
    "\n",
    "$$\n",
    "\\displaystyle \\min_p e(p) = \\min_p \\sum_i \\sum_{j>i} \\left|\\|p_i-p_j\\|^2-d_{ij}^2\\right|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolução do problema de otimização\n",
    "\n",
    "Podemos resolver este problema de algumas maneiras. A primeira delas abordadas será utilizando o método do gradient descent (em português, gradiente descendente).\n",
    "\n",
    "Para isto, precisamos calcular o gradiente de $e(p)$. Para facilitar os cálculos no momento, **o problema será simplificado para 1 dimensão.** Após uma primeira implementação, retornaremos para a abordagem 2D. Como estamos agora em 1D, $p_i \\in \\mathbb{R}$.\n",
    "\n",
    "$\\displaystyle \\nabla e(p) = \\left(\\frac{\\partial e(p)}{\\partial p_1}, \\frac{\\partial e(p)}{\\partial p_2}, \\dots, \\frac{\\partial e(p)}{\\partial p_n}\\right)$\n",
    "\n",
    "Vejamos então como calcular $\\displaystyle \\frac{\\partial e(p)}{\\partial p_k}$:\n",
    "\n",
    "$$\n",
    "\\displaystyle \\frac{\\partial e(p)}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left[\\sum_{j>k} \\left|(p_k-p_j)^2-d_{kj}^2\\right|^2 + \\sum_{k>i} \\left|(p_i-p_k)^2-d_{ik}^2\\right|^2\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\displaystyle \\frac{\\partial e(p)}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\sum_{i} \\left|(p_i-p_k)^2-d_{ik}^2\\right|^2 = 4 \\sum_i (p_i-p_k)\\left|(p_i-p_k)^2-d_{ik}^2\\right|\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "Este é um método iterativo que se aproxima cada vez mais de um mínimo local, fazendo uma dada configuração \"andar\" ou \"dar passos\" num sentido de decrescimento do valor da função. O passo iterativo é o seguinte:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "p^{(0)} = ? \\\\\n",
    "p^{(k+1)} = p^{(k)} - \\alpha\\nabla e(p^{(k)})\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Há duas perguntas imediatas a serem feitas:\n",
    "\n",
    "1) Que valor escolher para $p^{(0)}$?\n",
    "\n",
    "2) Que valor escolher para $\\alpha$, que indica o tamanho do passo?\n",
    "\n",
    "O valor de $p^{(0)}$ a princípio será escolhido aleatoriamente para cada uma das $n-1$ variáveis (visto que $p_1 = 0$).\n",
    "\n",
    "Já o passo $\\alpha$ será escolhido na mão, a partir de várias tentativas para a verificação da melhor convergência.\n",
    "\n",
    "Segue então o código implementando o método do gradient descent para $p_i \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 5 # número de pontos\n",
    "alpha = 0.1 # tamanho do passo\n",
    "tol = 1e-8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = [[.0, .7, .1, .4, .9],\n",
    "     [.7, .0, .4, .3, .6],\n",
    "     [.1, .4, .0, .8, .3],\n",
    "     [.4, .3, .8, .0, .5],\n",
    "     [.9, .6, .3, .5, .2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient(p):\n",
    "    grad = np.zeros(n)\n",
    "    for i in range(1, n):\n",
    "        for j in range(n):\n",
    "            diff = p[i] - p[j]\n",
    "            grad[i] += diff * ((diff**2) - D[i][j])\n",
    "        grad[i] *= 4\n",
    "    return grad\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def erro(p):\n",
    "    ret = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            ret += ((p[i]-p[j])**2 - D[i][j]**2)**2\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.41801925  0.11595963  0.75242584  0.0710389 ]\n",
      "[ 0.          0.7672003  -0.0169366   0.77237768  0.07609404] 1.13804024823\n"
     ]
    }
   ],
   "source": [
    "last = np.random.rand(n)\n",
    "last[0] = 0\n",
    "print(last)\n",
    "diff_norm = float('inf')\n",
    "\n",
    "while diff_norm > tol:\n",
    "    p = last - alpha * gradient(last)\n",
    "    diff_norm = np.linalg.norm(p-last)\n",
    "    last = p\n",
    "    \n",
    "print(last, erro(last))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
